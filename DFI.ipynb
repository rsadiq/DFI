{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DFI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOJkqw2sk0f4u5xf9NVW2OJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rsadiq/DFI/blob/master/DFI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CDTVzDPU4aK"
      },
      "source": [
        "\n",
        "1.   Git CLone the DFI Repo\n",
        "2.   Download the pretrained weights\n",
        "3.   Move weights to pretrained directory\n",
        "4.   Mount Your google Drive\n",
        "5.   Get the path of images from google Drive\n",
        "6.   Run for all the images in direcotry and save the results in the specified directory\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2N2pxLdWNp3D"
      },
      "source": [
        "!git clone https://github.com/backseason/DFI.git\n",
        "!cd DFI/\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wulbPvaCOHxx"
      },
      "source": [
        "import os\n",
        "os.chdir('DFI')\n",
        "!gdown --id 1N29cJghKKJOHbKgpwR2_Ui64umCE-XG3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u-7_NjiUmSP"
      },
      "source": [
        "!mkdir pretrained\n",
        "!mv dfi.pth pretrained/dfi.pth"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hp06b88Xi2L"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c1GkwTNvke_N"
      },
      "source": [
        "os.mkdir('/content/DFI/Everest')\n",
        "os.mkdir('/content/DFI/Everest/input_images')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cojmU1cmbNPM"
      },
      "source": [
        "!unzip \"/content/drive/My Drive/Everest/ing_hashtag_search_RO.zip\" \"-d\" \"/content/DFI/Everest/input_images\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Un_4UwsV4xbg"
      },
      "source": [
        "**ResNet Block**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COJ69ZQxXsru"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, dilation=1, downsample=None):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False) \n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        for i in self.bn1.parameters():\n",
        "            i.requires_grad = False\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n",
        "                               padding=dilation, dilation=dilation, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        for i in self.bn2.parameters():\n",
        "            i.requires_grad = False\n",
        "        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(planes * 4)\n",
        "        for i in self.bn3.parameters():\n",
        "            i.requires_grad = False\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv3(out)\n",
        "        out = self.bn3(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, layers):\n",
        "        self.freeze_bn = True\n",
        "        self.inplanes = 64\n",
        "        super(ResNet, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3,\n",
        "                               bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        for i in self.bn1.parameters():\n",
        "            i.requires_grad = False\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1, ceil_mode=True)\n",
        "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, layers[3], stride=1, dilation=2)\n",
        "\n",
        "    def _make_layer(self, block, planes, blocks, stride=1, dilation=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion or dilation == 2 or dilation == 4:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(self.inplanes, planes * block.expansion,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion),\n",
        "            )\n",
        "        for i in downsample._modules['1'].parameters():\n",
        "            i.requires_grad = False\n",
        "        layers = []\n",
        "        layers.append(block(self.inplanes, planes, stride,dilation=dilation, downsample=downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for i in range(1, blocks):\n",
        "            layers.append(block(self.inplanes, planes,dilation=dilation))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        tmp_x = []\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        tmp_x.append(x)\n",
        "        x = self.maxpool(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        tmp_x.append(x)\n",
        "        x = self.layer2(x)\n",
        "        tmp_x.append(x)\n",
        "        x = self.layer3(x)\n",
        "        tmp_x.append(x)\n",
        "        x = self.layer4(x)\n",
        "        tmp_x.append(x)\n",
        "\n",
        "        return tmp_x\n",
        "\n",
        "\n",
        "class ResNet_PPM(nn.Module):\n",
        "    def __init__(self, block, layers):\n",
        "        super(ResNet_PPM,self).__init__()\n",
        "        self.resnet = ResNet(block, layers)\n",
        "\n",
        "        self.in_planes = 256\n",
        "\n",
        "        self.ppm_pre = nn.Sequential(\n",
        "            nn.Conv2d(2048, self.in_planes, 1, 1, bias=False),\n",
        "            nn.GroupNorm(32, self.in_planes),\n",
        "        )\n",
        "        ppms = []\n",
        "        for ii in [1, 3, 5]:\n",
        "            ppms.append(nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(ii), \n",
        "                nn.Conv2d(self.in_planes, self.in_planes, 1, 1, bias=False), \n",
        "                nn.GroupNorm(32, self.in_planes),\n",
        "                ))\n",
        "        self.ppms = nn.ModuleList(ppms)\n",
        "\n",
        "        self.ppm_cat = nn.Sequential(\n",
        "            nn.Conv2d(self.in_planes, self.in_planes, 3, 1, 1, bias=False),\n",
        "            nn.GroupNorm(32, self.in_planes),\n",
        "        )\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resnet(x)\n",
        "\n",
        "        x_pre = self.ppm_pre(x[-1])\n",
        "        x_ppm = x_pre\n",
        "        for k in range(len(self.ppms)):\n",
        "            x_ppm = torch.add(x_ppm, F.interpolate(self.ppms[k](x_pre), x_pre.size()[2:], mode='bilinear', align_corners=True))\n",
        "        x_ppm = self.ppm_cat(self.relu(x_ppm))\n",
        "        x.append(x_ppm)\n",
        "\n",
        "        return x\n",
        "\n",
        "def resnet50_ppm():\n",
        "    model = ResNet_PPM(Bottleneck, [3, 4, 6, 3])\n",
        "    return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk3IBpUx5NH2"
      },
      "source": [
        "**DFI BLOCK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkTxnShk5Rfp"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from .resnet import resnet50_ppm\n",
        "\n",
        "config = {'converter': [[64,256,512,1024,2048,256],[32,64,128,256,256,256]], \n",
        "          'dfims': [[[32,64,128,256,256,256], 32, 0], [[32,64,128,256,256,256], 64, 1], [[32,64,128,256,256,256], 128, 2], [[32,64,128,256,256,256], 256, 3]], \n",
        "          'dfims_id': [[0,1,2,3,4,5], [0,1,2,3,4,5], [0,1,2,3,4,5], [0,1,2,3,4,5]], \n",
        "          'tams': [32, 64, 128, 256],\n",
        "          'predictors': [[[32, 64, 128, 256], True], [[32, 64, 128, 256], False], [[32, 64, 128, 256], True]], \n",
        "          'predictors_id': [0,1,2,3] } \n",
        "\n",
        "def gn(planes, channel_per_group=4, max_groups=32):\n",
        "    groups = planes // channel_per_group\n",
        "    return nn.GroupNorm(min(groups, max_groups), planes)\n",
        "\n",
        "class Converter(nn.Module):\n",
        "    def __init__(self, list_k):\n",
        "        super(Converter, self).__init__()\n",
        "        up = []\n",
        "        for i in range(len(list_k[0])):\n",
        "            up.append(nn.Sequential(\n",
        "                nn.Conv2d(list_k[0][i], list_k[1][i], 1, 1, bias=False), \n",
        "                gn(list_k[1][i]), \n",
        "                nn.ReLU(inplace=True),\n",
        "                ))\n",
        "        self.convert = nn.ModuleList(up)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = []\n",
        "        for i in range(len(x)):\n",
        "            out.append(self.convert[i](x[i]))\n",
        "        return out\n",
        "\n",
        "class DFIM(nn.Module): \n",
        "    def __init__(self, list_k, k, size_id, modes=3):\n",
        "        super(DFIM, self).__init__()\n",
        "        self.len = len(list_k)\n",
        "        self.size_id = size_id\n",
        "        up = []\n",
        "        for i in range(len(list_k)):\n",
        "            up.append(nn.Sequential(nn.Conv2d(list_k[i], k, 1, 1, bias=False), gn(k)))\n",
        "        self.merge = nn.ModuleList(up)\n",
        "        merge_convs, fcs, convs = [], [], []\n",
        "        for m in range(modes):\n",
        "            merge_convs.append(nn.Sequential(\n",
        "                        nn.Conv2d(k, k//4, 1, 1, bias=False), \n",
        "                        gn(k//4), \n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.Conv2d(k//4, k, 1, 1, bias=False),\n",
        "                        gn(k),\n",
        "                    ))\n",
        "            fcs.append(nn.Sequential(\n",
        "                    nn.Linear(k, k // 4, bias=False),\n",
        "                    nn.ReLU(inplace=True),\n",
        "                    nn.Linear(k // 4, self.len, bias=False),\n",
        "                ))\n",
        "            convs.append(nn.Sequential(nn.Conv2d(k, k, 3, 1, 1, bias=False), gn(k), nn.ReLU(inplace=True)))\n",
        "        self.merge_convs = nn.ModuleList(merge_convs)\n",
        "        self.fcs = nn.ModuleList(fcs)\n",
        "        self.convs = nn.ModuleList(convs)\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.relu =nn.ReLU(inplace=True)\n",
        "\n",
        "    def forward(self, list_x, mode=3):\n",
        "        x_size = list_x[self.size_id].size()\n",
        "        feas = []\n",
        "        for i in range(len(list_x)):\n",
        "            feas.append(self.merge[i](F.interpolate(list_x[i], x_size[2:], mode='bilinear', align_corners=True)).unsqueeze(dim=1))\n",
        "        feas = torch.cat(feas, dim=1) # Nx6xCxHxW\n",
        "        fea_sum = torch.sum(feas, dim=1) # NxCxHxW\n",
        "\n",
        "        if mode == 3:\n",
        "            outs = []\n",
        "            for mode_ in range(3):\n",
        "                fea_u = self.merge_convs[mode_](fea_sum)\n",
        "                fea_s = self.gap(fea_u).squeeze(-1).squeeze(-1) # NxC\n",
        "                fea_z = self.fcs[mode_](fea_s) # Nx6\n",
        "                selects = self.softmax(fea_z) # Nx6\n",
        "                feas_f = selects.reshape(x_size[0], self.len, 1, 1, 1).expand_as(feas) * feas # Nx6xCxHxW\n",
        "                _, index = torch.topk(selects, 3, dim=1) # Nx3\n",
        "                selected = []\n",
        "                for i in range(x_size[0]):\n",
        "                    selected.append(torch.index_select(feas_f, dim=1, index=index[i]))\n",
        "                selected = torch.cat(selected, dim=0)\n",
        "                fea_v = selected.sum(dim=1)\n",
        "                outs.append(self.convs[mode_](self.relu(fea_v)))\n",
        "            return torch.cat(outs, dim=0)\n",
        "        else:\n",
        "            fea_u = self.merge_convs[mode](fea_sum)\n",
        "            fea_s = self.gap(fea_u).squeeze(-1).squeeze(-1) # NxC\n",
        "            fea_z = self.fcs[mode](fea_s) # Nx6\n",
        "            selects = self.softmax(fea_z) # Nx6\n",
        "            feas_f = selects.reshape(x_size[0], self.len, 1, 1, 1).expand_as(feas) * feas # Nx6xCxHxW\n",
        "            _, index = torch.topk(selects, 3, dim=1) # Nx3\n",
        "            selected = []\n",
        "            for i in range(x_size[0]):\n",
        "                selected.append(torch.index_select(feas_f, dim=1, index=index[i]))\n",
        "            selected = torch.cat(selected, dim=0)\n",
        "            fea_v = selected.sum(dim=1)\n",
        "            return self.convs[mode](self.relu(fea_v))\n",
        "\n",
        "class TAM(nn.Module): # TAM\n",
        "    reduction = 4\n",
        "    def __init__(self, k):\n",
        "        super(TAM, self).__init__()\n",
        "        k_mid = int(k // self.reduction)\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Conv2d(k, k_mid, 1, 1, bias=False),\n",
        "            gn(k_mid),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(k_mid, k, 1, 1, bias=False),\n",
        "            gn(k),\n",
        "            nn.Sigmoid(),\n",
        "        )\n",
        "        self.block = nn.Sequential(nn.Conv2d(k, k, 3, 1, 1, bias=False), gn(k), nn.ReLU(inplace=True))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.attention(x)\n",
        "        out = torch.add(x, torch.mul(x, out))\n",
        "        out = self.block(out)\n",
        "        return out\n",
        "\n",
        "class Predictor(nn.Module):\n",
        "    def __init__(self, list_k, deep_sup):\n",
        "        super(Predictor, self).__init__()\n",
        "        self.trans = nn.ModuleList()\n",
        "        for i in range(len(list_k)):\n",
        "            self.trans.append(nn.Conv2d(list_k[i], 1, 1, 1))\n",
        "        self.fuse = nn.Conv2d(len(list_k), 1, 1, 1)\n",
        "        self.deep_sup = deep_sup\n",
        "\n",
        "    def forward(self, list_x, x_size=None):\n",
        "        up_x = []\n",
        "        for i, i_x in enumerate(list_x):\n",
        "            up_x.append(F.interpolate(self.trans[i](i_x), x_size[2:], mode='bilinear', align_corners=True))\n",
        "        fuse = self.fuse(torch.cat(up_x, dim = 1))\n",
        "        if self.deep_sup:\n",
        "            return [fuse, up_x]\n",
        "        else:\n",
        "            return [fuse]\n",
        "\n",
        "def extra_layer(base):\n",
        "    converter, dfims, tams, predictors = [], [], [], []\n",
        "    converter = Converter(config['converter'])\n",
        "\n",
        "    for k in config['dfims']:\n",
        "        dfims += [DFIM(k[0], k[1], k[2])]\n",
        "\n",
        "    for k in config['tams']:\n",
        "        tams += [TAM(k)]\n",
        "\n",
        "    for k in config['predictors']:\n",
        "        predictors += [Predictor(k[0], k[1])]\n",
        "\n",
        "    return base, converter, dfims, tams, predictors\n",
        "\n",
        "\n",
        "class DFI(nn.Module):\n",
        "    def __init__(self, base, converter, dfims, tams, predictors):\n",
        "        super(DFI, self).__init__()\n",
        "        self.dfims_id = config['dfims_id']\n",
        "        self.predictors_id = config['predictors_id']\n",
        "\n",
        "        self.base = base\n",
        "        self.converter = converter\n",
        "        self.dfims = nn.ModuleList(dfims)\n",
        "        self.tams = nn.ModuleList(tams)\n",
        "        self.predictors = nn.ModuleList(predictors)\n",
        "\n",
        "    def forward(self, x, mode = 3):\n",
        "        x_size = x.size()\n",
        "        x = self.converter(self.base(x))\n",
        "\n",
        "        # DFIM\n",
        "        dfims = []\n",
        "        for k in range(len(self.dfims)):\n",
        "           dfims.append(self.dfims[k]([x[i] for i in self.dfims_id[k]], mode=mode))\n",
        "    \n",
        "        # TAM\n",
        "        tams = []\n",
        "        for k in range(len(self.tams)):\n",
        "            if k in self.predictors_id:\n",
        "                tams.append(self.tams[k](dfims[k]))\n",
        "\n",
        "        # Prediction\n",
        "        predictions = []\n",
        "        if mode == 3:\n",
        "            for mode_ in range(mode):\n",
        "                predictions.append(self.predictors[mode_]([tam[mode_:mode_+1] for tam in tams], x_size))\n",
        "        else:\n",
        "            predictions = self.predictors[mode](tams, x_size)\n",
        "        return predictions\n",
        "\n",
        "def build_model():\n",
        "    return DFI(*extra_layer(resnet50_ppm()))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJrldIYv49Ds"
      },
      "source": [
        "**DataSet Class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73yDdWKR113T"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils import data\n",
        "from torchvision.transforms import functional as F\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "class ImageDataTest(data.Dataset):\n",
        "    def __init__(self,input, test_mode=1, sal_mode='e'):\n",
        "        if test_mode == 0:\n",
        "            self.image_root = './data/HED-BSDS_PASCAL/HED-BSDS/test/'\n",
        "            self.image_source = './data/HED-BSDS_PASCAL/HED-BSDS/test.lst'\n",
        "        elif test_mode == 1:\n",
        "            if sal_mode == 'e':\n",
        "                self.image_root = './data/ECSSD/Imgs/'\n",
        "                self.image_source = './data/ECSSD/test.lst'\n",
        "            elif sal_mode == 'p':\n",
        "                self.image_root = './data/PASCALS/Imgs/'\n",
        "                self.image_source = './data/PASCALS/test.lst'\n",
        "            elif sal_mode == 'd':\n",
        "                self.image_root = './data/DUTOMRON/Imgs/'\n",
        "                self.image_source = './data/DUTOMRON/test.lst'\n",
        "            elif sal_mode == 'h':\n",
        "                self.image_root = './data/HKU-IS/Imgs/'\n",
        "                self.image_source = './data/HKU-IS/test.lst'\n",
        "            elif sal_mode == 's':\n",
        "                self.image_root = './data/SOD/Imgs/'\n",
        "                self.image_source = './data/SOD/test.lst'\n",
        "            elif sal_mode == 't':\n",
        "                self.image_root = './data/DUTS-TE/Imgs/'\n",
        "                self.image_source = './data/DUTS-TE/test.lst'\n",
        "        elif test_mode == 2:\n",
        "            self.image_root = './data/SK-LARGE/images/test/'\n",
        "            self.image_source = './data/SK-LARGE/test.lst'\n",
        "        elif test_mode == 3:\n",
        "            self.image_root = './demo/images/'\n",
        "            self.image_source = './demo/img.lst'\n",
        "\n",
        "        elif test_mode == 4:\n",
        "            self.image_root = ''\n",
        "\n",
        "        if not test_mode ==4:\n",
        "            with open(self.image_source, 'r') as f:\n",
        "                self.image_list = [x.strip() for x in f.readlines()]\n",
        "            self.image_num = len(self.image_list)\n",
        "        else:\n",
        "            self.image_list = [input]\n",
        "            self.image_num = len(self.image_list)\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        image, im_size = load_image_test(os.path.join(self.image_root, self.image_list[item]))\n",
        "        image = torch.Tensor(image)\n",
        "        return {'image': image, 'name': self.image_list[item%self.image_num], 'size': im_size}\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.image_num\n",
        "\n",
        "def get_loader(input, test_mode=0, sal_mode='e', pin=False):\n",
        "    dataset = ImageDataTest(input,test_mode=test_mode, sal_mode=sal_mode)\n",
        "    data_loader = data.DataLoader(dataset=dataset, batch_size=1, shuffle=False, num_workers=1,\n",
        "                                      pin_memory=pin)\n",
        "    return data_loader\n",
        "\n",
        "def load_image_test(pah):\n",
        "    if not os.path.exists(pah):\n",
        "        print('File Not Exists')\n",
        "    im = cv2.imread(pah)\n",
        "    in_ = np.array(im, dtype=np.float32)\n",
        "    im_size = tuple(in_.shape[:2])\n",
        "    in_ -= np.array((104.00699, 116.66877, 122.67892))\n",
        "    in_ = in_.transpose((2,0,1))\n",
        "    return in_, im_size"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFXqC69P5GjX"
      },
      "source": [
        "**SOLVER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4c-larj184z"
      },
      "source": [
        "import torch\n",
        "from torch.nn import utils, functional as F\n",
        "from networks.dfi import build_model\n",
        "import numpy as np\n",
        "import os\n",
        "import cv2\n",
        "import ntpath\n",
        "\n",
        "class Solver():\n",
        "    def __init__(self, config):\n",
        "        # self.data_loader = data_loader\n",
        "        self.config = config\n",
        "        self.net = build_model()\n",
        "        print('Loading pre-trained model from %s...' % self.config.model)\n",
        "        self.net = self.net.cpu()\n",
        "        self.net.load_state_dict(torch.load(self.config.model))\n",
        "        if self.config.cuda:\n",
        "            self.net = self.net.cuda()\n",
        "        torch.cuda.empty_cache()\n",
        "        self.net.eval()\n",
        "\n",
        "    def test(self, data_loader):\n",
        "        test_mode = 4\n",
        "        mode_name = ['edge', 'sal', 'skel']\n",
        "        EPSILON = 1e-8\n",
        "        img_num = len(data_loader)\n",
        "        for i, data_batch in enumerate(data_loader):\n",
        "            images, name, im_size = data_batch['image'], data_batch['name'][0], np.asarray(data_batch['size'])\n",
        "            if test_mode == 0: # edge task\n",
        "                images = images.numpy()[0].transpose((1,2,0))\n",
        "                scale = [0.5, 1, 1.5, 2] # multi-scale testing as commonly done\n",
        "                multi_fuse = np.zeros(im_size, np.float32)\n",
        "                for k in range(0, len(scale)):\n",
        "                    im_ = cv2.resize(images, None, fx=scale[k], fy=scale[k], interpolation=cv2.INTER_LINEAR)\n",
        "                    im_ = im_.transpose((2, 0, 1))\n",
        "                    im_ = torch.Tensor(im_[np.newaxis, ...])\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        if self.config.cuda:\n",
        "                            im_ = im_.cuda()\n",
        "                        preds = self.net(im_, mode=test_mode)\n",
        "                        preds_i = []\n",
        "                        for p in preds[1]:\n",
        "                            preds_i.append(np.squeeze(torch.sigmoid(p).cpu().data.numpy()))\n",
        "                        pred_fuse = np.squeeze(torch.sigmoid(preds[0]).cpu().data.numpy())\n",
        "                        pred = (pred_fuse + sum(preds_i)) / (1.0 + len(preds_i))\n",
        "\n",
        "                        pred = (pred - np.min(pred) + EPSILON) / (np.max(pred) - np.min(pred) + EPSILON)\n",
        "\n",
        "                        pred = cv2.resize(pred, (im_size[1], im_size[0]), interpolation=cv2.INTER_LINEAR)\n",
        "                        multi_fuse += pred\n",
        "\n",
        "                multi_fuse /= len(scale)\n",
        "                multi_fuse = 255 * (1 - multi_fuse)\n",
        "                cv2.imwrite(os.path.join(self.config.test_fold, name[:-4] + '_' + mode_name[test_mode] + '.png'), multi_fuse)\n",
        "\n",
        "            elif test_mode == 1: # saliency task\n",
        "                with torch.no_grad():\n",
        "                    if self.config.cuda:\n",
        "                        images = images.cuda()\n",
        "                    preds = self.net(images, mode=test_mode)\n",
        "                    pred = np.squeeze(torch.sigmoid(preds[0]).cpu().data.numpy())\n",
        "\n",
        "                    multi_fuse = 255 * pred\n",
        "                    cv2.imwrite(os.path.join(self.config.test_fold, name[:-4] + '_' + mode_name[test_mode] + '.png'), multi_fuse)\n",
        "\n",
        "            elif test_mode == 2: # skeleton task\n",
        "                images = images.numpy()[0].transpose((1,2,0))\n",
        "                scale = [0.5, 1, 1.5] # multi-scale testing as commonly done\n",
        "                multi_fuse = np.zeros(im_size, np.float32)\n",
        "                for k in range(0, len(scale)):\n",
        "                    im_ = cv2.resize(images, None, fx=scale[k], fy=scale[k], interpolation=cv2.INTER_LINEAR)\n",
        "                    im_ = im_.transpose((2, 0, 1))\n",
        "                    im_ = torch.Tensor(im_[np.newaxis, ...])\n",
        "\n",
        "                    with torch.no_grad():\n",
        "                        if self.config.cuda:\n",
        "                            im_ = im_.cuda()\n",
        "                        preds = self.net(im_, mode=test_mode)\n",
        "                        pred_fuse = np.squeeze(torch.sigmoid(preds[0]).cpu().data.numpy())\n",
        "\n",
        "                        pred = pred_fuse\n",
        "                        pred = (pred - np.min(pred) + EPSILON) / (np.max(pred) - np.min(pred) + EPSILON)\n",
        "\n",
        "                        pred = cv2.resize(pred, (im_size[1], im_size[0]), interpolation=cv2.INTER_LINEAR)\n",
        "                        multi_fuse += pred\n",
        "\n",
        "                multi_fuse /= len(scale)\n",
        "                multi_fuse = 255 * (1 - multi_fuse)\n",
        "                cv2.imwrite(os.path.join(self.config.test_fold, name[:-4] + '_' + mode_name[test_mode] + '.png'), multi_fuse)\n",
        "            elif test_mode == 3: # all tasks\n",
        "                with torch.no_grad():\n",
        "                    if self.config.cuda:\n",
        "                        images = images.cuda()\n",
        "                    preds = self.net(images, mode=test_mode)\n",
        "                    pred_edge = np.squeeze(torch.sigmoid(preds[0][0]).cpu().data.numpy())\n",
        "                    pred_sal = np.squeeze(torch.sigmoid(preds[1][0]).cpu().data.numpy())\n",
        "                    pred_skel = np.squeeze(torch.sigmoid(preds[2][0]).cpu().data.numpy())\n",
        "\n",
        "                    pred_edge = (pred_edge - np.min(pred_edge) + EPSILON) / (np.max(pred_edge) - np.min(pred_edge) + EPSILON)\n",
        "                    pred_skel = (pred_skel - np.min(pred_skel) + EPSILON) / (np.max(pred_skel) - np.min(pred_skel) + EPSILON)\n",
        "\n",
        "                    cv2.imwrite(os.path.join(self.config.test_fold, name[:-4] + '_' + mode_name[0] + '.png'), 255 * (1 - pred_edge))\n",
        "                    cv2.imwrite(os.path.join(self.config.test_fold, name[:-4] + '_' + mode_name[1] + '.png'), 255 * pred_sal)\n",
        "                    cv2.imwrite(os.path.join(self.config.test_fold, name[:-4] + '_' + mode_name[2] + '.png'), 255 * (1 - pred_skel))\n",
        "\n",
        "            elif test_mode == 4:  # all tasks\n",
        "                with torch.no_grad():\n",
        "                    if self.config.cuda:\n",
        "                        images = images.cuda()\n",
        "                    preds = self.net(images, mode=3)\n",
        "                    pred_edge = np.squeeze(torch.sigmoid(preds[0][0]).cpu().data.numpy())\n",
        "                    pred_sal = np.squeeze(torch.sigmoid(preds[1][0]).cpu().data.numpy())\n",
        "                    pred_skel = np.squeeze(torch.sigmoid(preds[2][0]).cpu().data.numpy())\n",
        "\n",
        "                    pred_edge = (pred_edge - np.min(pred_edge) + EPSILON) / (np.max(pred_edge) - np.min(pred_edge) + EPSILON)\n",
        "                    pred_skel = (pred_skel - np.min(pred_skel) + EPSILON) / (np.max(pred_skel) - np.min(pred_skel) + EPSILON)\n",
        "                    # pred_sal = (pred_sal - np.min(pred_sal) + EPSILON) / (np.max(pred_sal) - np.min(pred_sal) + EPSILON)\n",
        "\n",
        "                    name = ntpath.basename(name)\n",
        "                    cv2.imwrite(os.path.join(self.config.test_fold, name[:-4] + '_' + mode_name[0] + '.png'),\n",
        "                                255 * (1 - pred_edge))\n",
        "                    cv2.imwrite(os.path.join(self.config.test_fold, name[:-4] + '_' + mode_name[1] + '.png'), 255 * pred_sal)\n",
        "                    cv2.imwrite(os.path.join(self.config.test_fold, name[:-4] + '_' + mode_name[2] + '.png'),\n",
        "                                255 * (1 - pred_skel))\n",
        "\n",
        "\n",
        "        # print('Predicition of DFI Finished.')\n",
        "        return pred_sal,pred_skel,pred_edge\n",
        "\n"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXKXiaFOgQA2"
      },
      "source": [
        "class DFI_Namespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n",
        "def convert_arg_line_to_args(arg_line):\n",
        "    for arg in arg_line.split():\n",
        "        if not arg.strip():\n",
        "            continue\n",
        "        yield arg\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFPeIAK-fgws"
      },
      "source": [
        "from tqdm import tqdm\n",
        "import os \n",
        "\n",
        "img_path = '/content/DFI/Everest/input_images/RO'\n",
        "outpath = '/content/drive/My Drive/Everest/Ing_hasttags_RO_DFI'\n",
        "image_files = os.listdir(img_path)\n",
        "processed_files = os.listdir(outpath)\n",
        "newfiles_list = []\n",
        "for files in image_files:\n",
        "    fn, ext = os.path.splitext(files)\n",
        "    new_fn = fn+'_edge.png'\n",
        "    if new_fn not in processed_files:\n",
        "        newfiles_list.append(files)\n",
        "print(len(newfiles_list),len(image_files))\n",
        "config = DFI_Namespace(cuda=True, in_image='demo/images/ILSVRC2012_test_00002969.jpg',\n",
        "                           model='pretrained/dfi.pth', sal_mode='e', \n",
        "                       test_fold=outpath,\n",
        "                       test_mode=4)\n",
        "# # if not os.path.exists(config.test_fold): os.mkdir(config.test_fold)\n",
        "test = Solver(config)\n",
        "for img in tqdm(newfiles_list):\n",
        "    file = os.path.join(img_path,img)\n",
        "    config.in_image = file\n",
        "    data_loader = get_loader(config.in_image,test_mode=config.test_mode, sal_mode=config.sal_mode)\n",
        "    DFI_sal,DFI_edge,DFI_skel = test.test(data_loader)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}